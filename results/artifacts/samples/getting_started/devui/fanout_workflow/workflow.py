# Copyright (c) Microsoft. All rights reserved.

"""è¤‡é›‘ãªFan-In/Fan-Outãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã€‚

ã“ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¯è¤‡æ•°ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’æŒã¤é«˜åº¦ãªãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ç¤ºã—ã¦ã„ã¾ã™ï¼š
1. ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ - è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
2. ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ - è¤‡æ•°ã®ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼ãŒä¸¦åˆ—ã§ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’ãƒã‚§ãƒƒã‚¯
3. ãƒ‡ãƒ¼ã‚¿å¤‰æ› - ç•°ãªã‚‹å¤‰æ›ãƒ—ãƒ­ã‚»ãƒƒã‚µã¸ã®ãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆ
4. å“è³ªä¿è¨¼ - è¤‡æ•°ã®QAãƒã‚§ãƒƒã‚¯ãŒä¸¦åˆ—ã§å®Ÿè¡Œ
5. ãƒ‡ãƒ¼ã‚¿é›†ç´„ - å‡¦ç†çµæœã‚’ãƒ•ã‚¡ãƒ³ã‚¤ãƒ³ã§çµåˆ
6. æœ€çµ‚å‡¦ç† - ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Œäº†

ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«ã¯å®Ÿéš›ã®å‡¦ç†æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ç¾å®Ÿçš„ãªé…å»¶ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€
æ¡ä»¶ä»˜ãå‡¦ç†ã‚’ä¼´ã†è¤‡é›‘ãªãƒ•ã‚¡ãƒ³ã‚¤ãƒ³/ãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
"""

import asyncio
import logging
from dataclasses import dataclass
from enum import Enum
from typing import Literal

from agent_framework import (
    Executor,
    WorkflowBuilder,
    WorkflowContext,
    handler,
)
from pydantic import BaseModel, Field
from typing_extensions import Never


class DataType(Enum):
    """å‡¦ç†ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã®ç¨®é¡ã€‚"""

    CUSTOMER = "customer"
    TRANSACTION = "transaction"
    PRODUCT = "product"
    ANALYTICS = "analytics"


class ValidationResult(Enum):
    """ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã®çµæœã€‚"""

    VALID = "valid"
    WARNING = "warning"
    ERROR = "error"


class ProcessingRequest(BaseModel):
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®è¤‡é›‘ãªå…¥åŠ›æ§‹é€ ã€‚"""

    # åŸºæœ¬æƒ…å ±
    data_source: Literal["database", "api", "file_upload", "streaming"] = Field(
        description="The source of the data to be processed", default="database"
    )

    data_type: Literal["customer", "transaction", "product", "analytics"] = Field(
        description="Type of data being processed", default="customer"
    )

    processing_priority: Literal["low", "normal", "high", "critical"] = Field(
        description="Processing priority level", default="normal"
    )

    # å‡¦ç†è¨­å®š
    batch_size: int = Field(description="Number of records to process in each batch", default=500, ge=100, le=10000)

    quality_threshold: float = Field(
        description="Minimum quality score required (0.0-1.0)", default=0.8, ge=0.0, le=1.0
    )

    # æ¤œè¨¼è¨­å®š
    enable_schema_validation: bool = Field(description="Enable schema validation checks", default=True)

    enable_security_validation: bool = Field(description="Enable security validation checks", default=True)

    enable_quality_validation: bool = Field(description="Enable data quality validation checks", default=True)

    # å¤‰æ›ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    transformations: list[Literal["normalize", "enrich", "aggregate"]] = Field(
        description="List of transformations to apply", default=["normalize", "enrich"]
    )

    # ä»»æ„ã®èª¬æ˜
    description: str | None = Field(description="Optional description of the processing request", default=None)

    # ãƒ†ã‚¹ãƒˆå¤±æ•—ã‚·ãƒŠãƒªã‚ª
    force_validation_failure: bool = Field(
        description="Force validation failure for testing (demo purposes)", default=False
    )

    force_transformation_failure: bool = Field(
        description="Force transformation failure for testing (demo purposes)", default=False
    )


@dataclass
class DataBatch:
    """å‡¦ç†ä¸­ã®ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒã‚’è¡¨ã™ã€‚"""

    batch_id: str
    data_type: DataType
    size: int
    content: str
    source: str = "unknown"
    timestamp: float = 0.0


@dataclass
class ValidationReport:
    """ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã®ãƒ¬ãƒãƒ¼ãƒˆã€‚"""

    batch_id: str
    validator_id: str
    result: ValidationResult
    issues_found: int
    processing_time: float
    details: str


@dataclass
class TransformationResult:
    """ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã®çµæœã€‚"""

    batch_id: str
    transformer_id: str
    original_size: int
    processed_size: int
    transformation_type: str
    processing_time: float
    success: bool


@dataclass
class QualityAssessment:
    """å“è³ªè©•ä¾¡çµæœã€‚"""

    batch_id: str
    assessor_id: str
    quality_score: float
    recommendations: list[str]
    processing_time: float


@dataclass
class ProcessingSummary:
    """ã™ã¹ã¦ã®å‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¸ã®æ¦‚è¦ã€‚"""

    batch_id: str
    total_processing_time: float
    validation_reports: list[ValidationReport]
    transformation_results: list[TransformationResult]
    quality_assessments: list[QualityAssessment]
    final_status: str


# ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ã‚¹ãƒ†ãƒ¼ã‚¸
class DataIngestion(Executor):
    """è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ã‚’é…å»¶ä»˜ãã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã€‚"""

    @handler
    async def ingest_data(self, request: ProcessingRequest, ctx: WorkflowContext[DataBatch]) -> None:
        """å…¥åŠ›è¨­å®šã«åŸºã¥ã„ãŸç¾å®Ÿçš„ãªé…å»¶ã§ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã€‚"""
        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã«åŸºã¥ããƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é…å»¶ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        delay_map = {"database": 1.5, "api": 3.0, "file_upload": 4.0, "streaming": 1.0}
        delay = delay_map.get(request.data_source, 3.0)
        await asyncio.sleep(delay)  # ãƒ‡ãƒ¢ç”¨ã®å›ºå®šé…å»¶

        # å„ªå…ˆåº¦ã¨è¨­å®šã«åŸºã¥ããƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        base_size = request.batch_size
        if request.processing_priority == "critical":
            size_multiplier = 1.7  # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«å„ªå…ˆåº¦ã¯æœ€å¤§ã®ãƒãƒƒãƒã‚’å–å¾—
        elif request.processing_priority == "high":
            size_multiplier = 1.3  # é«˜å„ªå…ˆåº¦ã¯å¤§ãã‚ã®ãƒãƒƒãƒã‚’å–å¾—
        elif request.processing_priority == "low":
            size_multiplier = 0.6  # ä½å„ªå…ˆåº¦ã¯å°ã•ã‚ã®ãƒãƒƒãƒã‚’å–å¾—
        else:  # normal
            size_multiplier = 1.0  # é€šå¸¸å„ªå…ˆåº¦ã¯åŸºæœ¬ã‚µã‚¤ã‚ºã‚’ä½¿ç”¨

        actual_size = int(base_size * size_multiplier)

        batch = DataBatch(
            batch_id=f"batch_{5555}",  # Fixed batch ID for demo
            data_type=DataType(request.data_type),
            size=actual_size,
            content=f"Processing {request.data_type} data from {request.data_source}",
            source=request.data_source,
            timestamp=asyncio.get_event_loop().time(),
        )

        # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã¨å…ƒã®Requestã®ä¸¡æ–¹ã‚’å…±æœ‰Stateã«ä¿å­˜
        await ctx.set_shared_state(f"batch_{batch.batch_id}", batch)
        await ctx.set_shared_state(f"request_{batch.batch_id}", request)

        await ctx.send_message(batch)


# æ¤œè¨¼ã‚¹ãƒ†ãƒ¼ã‚¸ï¼ˆãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆï¼‰
class SchemaValidator(Executor):
    """ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚­ãƒ¼ãƒã¨æ§‹é€ ã‚’æ¤œè¨¼ã€‚"""

    @handler
    async def validate_schema(self, batch: DataBatch, ctx: WorkflowContext[ValidationReport]) -> None:
        """å‡¦ç†é…å»¶ã‚’ä¼´ã†ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼ã‚’å®Ÿè¡Œã€‚"""
        # ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
        request = await ctx.get_shared_state(f"request_{batch.batch_id}")
        if not request or not request.enable_schema_validation:
            return

        # ã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        processing_time = 2.0  # å›ºå®šå‡¦ç†æ™‚é–“
        await asyncio.sleep(processing_time)

        # æ¤œè¨¼çµæœã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ - å¼·åˆ¶å¤±æ•—ãƒ•ãƒ©ã‚°ã‚’è€ƒæ…®
        issues = 4 if request.force_validation_failure else 2  # å›ºå®šã®å•é¡Œæ•°

        result = (
            ValidationResult.VALID
            if issues <= 1
            else (ValidationResult.WARNING if issues <= 2 else ValidationResult.ERROR)
        )

        report = ValidationReport(
            batch_id=batch.batch_id,
            validator_id=self.id,
            result=result,
            issues_found=issues,
            processing_time=processing_time,
            details=f"Schema validation found {issues} issues in {batch.data_type.value} data from {batch.source}",
        )

        await ctx.send_message(report)


class DataQualityValidator(Executor):
    """ãƒ‡ãƒ¼ã‚¿ã®å“è³ªã¨å®Œå…¨æ€§ã‚’æ¤œè¨¼ã€‚"""

    @handler
    async def validate_quality(self, batch: DataBatch, ctx: WorkflowContext[ValidationReport]) -> None:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼ã‚’å®Ÿè¡Œã€‚"""
        # å“è³ªæ¤œè¨¼ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
        request = await ctx.get_shared_state(f"request_{batch.batch_id}")
        if not request or not request.enable_quality_validation:
            return

        processing_time = 2.5  # å›ºå®šå‡¦ç†æ™‚é–“
        await asyncio.sleep(processing_time)

        # é«˜å„ªå…ˆåº¦ãƒ‡ãƒ¼ã‚¿ã¯ã‚ˆã‚Šå³ã—ã„å“è³ªãƒã‚§ãƒƒã‚¯
        issues = (
            2  # Fixed issue count for high priority
            if request.processing_priority in ["critical", "high"]
            else 3  # Fixed issue count for normal priority
        )

        if request.force_validation_failure:
            issues = max(issues, 4)  # å¤±æ•—ã‚’ç¢ºå®Ÿã«ã™ã‚‹

        result = (
            ValidationResult.VALID
            if issues <= 1
            else (ValidationResult.WARNING if issues <= 3 else ValidationResult.ERROR)
        )

        report = ValidationReport(
            batch_id=batch.batch_id,
            validator_id=self.id,
            result=result,
            issues_found=issues,
            processing_time=processing_time,
            details=f"Quality check found {issues} data quality issues (priority: {request.processing_priority})",
        )

        await ctx.send_message(report)


class SecurityValidator(Executor):
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ã®å•é¡Œã‚’æ¤œè¨¼ã€‚"""

    @handler
    async def validate_security(self, batch: DataBatch, ctx: WorkflowContext[ValidationReport]) -> None:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ¤œè¨¼ã‚’å®Ÿè¡Œã€‚"""
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ¤œè¨¼ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
        request = await ctx.get_shared_state(f"request_{batch.batch_id}")
        if not request or not request.enable_security_validation:
            return

        processing_time = 3.0  # å›ºå®šå‡¦ç†æ™‚é–“
        await asyncio.sleep(processing_time)

        # é¡§å®¢/å–å¼•ãƒ‡ãƒ¼ã‚¿ã¯ã‚ˆã‚Šå³æ ¼ãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
        issues = 1 if batch.data_type in [DataType.CUSTOMER, DataType.TRANSACTION] else 2

        if request.force_validation_failure:
            issues = max(issues, 1)  # å°‘ãªãã¨ã‚‚1ã¤ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œã‚’å¼·åˆ¶

        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¨ãƒ©ãƒ¼ã¯ã‚ˆã‚Šæ·±åˆ»ã§è¨±å®¹åº¦ãŒä½ã„
        result = ValidationResult.VALID if issues == 0 else ValidationResult.ERROR

        report = ValidationReport(
            batch_id=batch.batch_id,
            validator_id=self.id,
            result=result,
            issues_found=issues,
            processing_time=processing_time,
            details=f"Security scan found {issues} security issues in {batch.data_type.value} data",
        )

        await ctx.send_message(report)


# æ¤œè¨¼é›†ç´„å™¨ï¼ˆãƒ•ã‚¡ãƒ³ã‚¤ãƒ³ï¼‰
class ValidationAggregator(Executor):
    """æ¤œè¨¼çµæœã‚’é›†ç´„ã—æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ±ºå®šã€‚"""

    @handler
    async def aggregate_validations(
        self, reports: list[ValidationReport], ctx: WorkflowContext[DataBatch, str]
    ) -> None:
        """ã™ã¹ã¦ã®æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’é›†ç´„ã—å‡¦ç†åˆ¤æ–­ã‚’è¡Œã†ã€‚"""
        if not reports:
            return

        batch_id = reports[0].batch_id
        request = await ctx.get_shared_state(f"request_{batch_id}")

        await asyncio.sleep(1)  # é›†ç´„å‡¦ç†æ™‚é–“

        total_issues = sum(report.issues_found for report in reports)
        has_errors = any(report.result == ValidationResult.ERROR for report in reports)

        # å“è³ªã‚¹ã‚³ã‚¢ï¼ˆ0.0ã‹ã‚‰1.0ï¼‰ã‚’è¨ˆç®—
        max_possible_issues = len(reports) * 5  # ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼ã”ã¨ã«æœ€å¤§5ä»¶ã®å•é¡Œã‚’æƒ³å®š
        quality_score = max(0.0, 1.0 - (total_issues / max_possible_issues))

        # åˆ¤æ–­ãƒ­ã‚¸ãƒƒã‚¯ï¼šã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹ã‹å“è³ªãŒé–¾å€¤ä»¥ä¸‹ãªã‚‰å¤±æ•—
        should_fail = has_errors or (quality_score < request.quality_threshold)

        if should_fail:
            failure_reason: list[str] = []
            if has_errors:
                failure_reason.append("validation errors detected")
            if quality_score < request.quality_threshold:
                failure_reason.append(
                    f"quality score {quality_score:.2f} below threshold {request.quality_threshold:.2f}"
                )

            reason = " and ".join(failure_reason)
            await ctx.yield_output(
                f"Batch {batch_id} failed validation: {reason}. "
                f"Total issues: {total_issues}, Quality score: {quality_score:.2f}"
            )
            return

        # å…±æœ‰Stateã‹ã‚‰å…ƒã®ãƒãƒƒãƒã‚’å–å¾—
        batch_data = await ctx.get_shared_state(f"batch_{batch_id}")
        if batch_data:
            await ctx.send_message(batch_data)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç°¡ç•¥åŒ–ã—ãŸãƒãƒƒãƒã‚’ä½œæˆ
            batch = DataBatch(
                batch_id=batch_id,
                data_type=DataType.ANALYTICS,
                size=500,
                content="Validated data ready for transformation",
            )
            await ctx.send_message(batch)


# å¤‰æ›ã‚¹ãƒ†ãƒ¼ã‚¸ï¼ˆãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆï¼‰
class DataNormalizer(Executor):
    """ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–ã—ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã€‚"""

    @handler
    async def normalize_data(self, batch: DataBatch, ctx: WorkflowContext[TransformationResult]) -> None:
        """ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–ã‚’å®Ÿè¡Œã€‚"""
        request = await ctx.get_shared_state(f"request_{batch.batch_id}")

        # æ­£è¦åŒ–ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
        if not request or "normalize" not in request.transformations:
            # "skipped"çµæœã‚’é€ä¿¡
            result = TransformationResult(
                batch_id=batch.batch_id,
                transformer_id=self.id,
                original_size=batch.size,
                processed_size=batch.size,
                transformation_type="normalization",
                processing_time=0.1,
                success=True,  # Consider skipped as successful
            )
            await ctx.send_message(result)
            return

        processing_time = 4.0  # å›ºå®šå‡¦ç†æ™‚é–“
        await asyncio.sleep(processing_time)

        # æ­£è¦åŒ–ä¸­ã®ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºå¤‰åŒ–ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        processed_size = int(batch.size * 1.0)  # ãƒ‡ãƒ¢ç”¨ã«ã‚µã‚¤ã‚ºå¤‰æ›´ãªã—

        # å¼·åˆ¶å¤±æ•—ãƒ•ãƒ©ã‚°ã‚’è€ƒæ…®
        success = not request.force_transformation_failure  # 75%æˆåŠŸç‡ã‚’å¸¸ã«æˆåŠŸã«ç°¡ç•¥åŒ–

        result = TransformationResult(
            batch_id=batch.batch_id,
            transformer_id=self.id,
            original_size=batch.size,
            processed_size=processed_size,
            transformation_type="normalization",
            processing_time=processing_time,
            success=success,
        )

        await ctx.send_message(result)


class DataEnrichment(Executor):
    """ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ æƒ…å ±ã§å¼·åŒ–ã€‚"""

    @handler
    async def enrich_data(self, batch: DataBatch, ctx: WorkflowContext[TransformationResult]) -> None:
        """ãƒ‡ãƒ¼ã‚¿å¼·åŒ–ã‚’å®Ÿè¡Œã€‚"""
        request = await ctx.get_shared_state(f"request_{batch.batch_id}")

        # å¼·åŒ–ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
        if not request or "enrich" not in request.transformations:
            # "skipped"çµæœã‚’é€ä¿¡
            result = TransformationResult(
                batch_id=batch.batch_id,
                transformer_id=self.id,
                original_size=batch.size,
                processed_size=batch.size,
                transformation_type="enrichment",
                processing_time=0.1,
                success=True,  # Consider skipped as successful
            )
            await ctx.send_message(result)
            return

        processing_time = 5.0  # å›ºå®šå‡¦ç†æ™‚é–“
        await asyncio.sleep(processing_time)

        processed_size = int(batch.size * 1.3)  # å¼·åŒ–ã¯ãƒ‡ãƒ¼ã‚¿ã‚’å¢—åŠ ã•ã›ã‚‹

        # å¼·åˆ¶å¤±æ•—ãƒ•ãƒ©ã‚°ã‚’è€ƒæ…®
        success = not request.force_transformation_failure  # 67%æˆåŠŸç‡ã‚’å¸¸ã«æˆåŠŸã«ç°¡ç•¥åŒ–

        result = TransformationResult(
            batch_id=batch.batch_id,
            transformer_id=self.id,
            original_size=batch.size,
            processed_size=processed_size,
            transformation_type="enrichment",
            processing_time=processing_time,
            success=success,
        )

        await ctx.send_message(result)


class DataAggregator(Executor):
    """ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ã—è¦ç´„ã€‚"""

    @handler
    async def aggregate_data(self, batch: DataBatch, ctx: WorkflowContext[TransformationResult]) -> None:
        """ãƒ‡ãƒ¼ã‚¿é›†ç´„ã‚’å®Ÿè¡Œã€‚"""
        request = await ctx.get_shared_state(f"request_{batch.batch_id}")

        # é›†ç´„ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
        if not request or "aggregate" not in request.transformations:
            # "skipped"çµæœã‚’é€ä¿¡
            result = TransformationResult(
                batch_id=batch.batch_id,
                transformer_id=self.id,
                original_size=batch.size,
                processed_size=batch.size,
                transformation_type="aggregation",
                processing_time=0.1,
                success=True,  # Consider skipped as successful
            )
            await ctx.send_message(result)
            return

        processing_time = 2.5  # å›ºå®šå‡¦ç†æ™‚é–“
        await asyncio.sleep(processing_time)

        processed_size = int(batch.size * 0.5)  # é›†ç´„ã¯ãƒ‡ãƒ¼ã‚¿ã‚’æ¸›å°‘ã•ã›ã‚‹

        # å¼·åˆ¶å¤±æ•—ãƒ•ãƒ©ã‚°ã‚’è€ƒæ…®
        success = not request.force_transformation_failure  # 80%æˆåŠŸç‡ã‚’å¸¸ã«æˆåŠŸã«ç°¡ç•¥åŒ–

        result = TransformationResult(
            batch_id=batch.batch_id,
            transformer_id=self.id,
            original_size=batch.size,
            processed_size=processed_size,
            transformation_type="aggregation",
            processing_time=processing_time,
            success=success,
        )

        await ctx.send_message(result)


# å“è³ªä¿è¨¼ã‚¹ãƒ†ãƒ¼ã‚¸ï¼ˆãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆï¼‰
class PerformanceAssessor(Executor):
    """å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰¹æ€§ã‚’è©•ä¾¡ã€‚"""

    @handler
    async def assess_performance(
        self, results: list[TransformationResult], ctx: WorkflowContext[QualityAssessment]
    ) -> None:
        """å¤‰æ›ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã€‚"""
        if not results:
            return

        batch_id = results[0].batch_id

        processing_time = 2.0  # å›ºå®šå‡¦ç†æ™‚é–“
        await asyncio.sleep(processing_time)

        avg_processing_time = sum(r.processing_time for r in results) / len(results)
        success_rate = sum(1 for r in results if r.success) / len(results)

        quality_score = (success_rate * 0.7 + (1 - min(avg_processing_time / 10, 1)) * 0.3) * 100

        recommendations: list[str] = []
        if success_rate < 0.8:
            recommendations.append("Consider improving transformation reliability")
        if avg_processing_time > 5:
            recommendations.append("Optimize processing performance")
        if quality_score < 70:
            recommendations.append("Review overall data pipeline efficiency")

        assessment = QualityAssessment(
            batch_id=batch_id,
            assessor_id=self.id,
            quality_score=quality_score,
            recommendations=recommendations,
            processing_time=processing_time,
        )

        await ctx.send_message(assessment)


class AccuracyAssessor(Executor):
    """å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®æ­£ç¢ºæ€§ã¨å¦¥å½“æ€§ã‚’è©•ä¾¡ã€‚"""

    @handler
    async def assess_accuracy(
        self, results: list[TransformationResult], ctx: WorkflowContext[QualityAssessment]
    ) -> None:
        """å¤‰æ›ã®æ­£ç¢ºæ€§ã‚’è©•ä¾¡ã€‚"""
        if not results:
            return

        batch_id = results[0].batch_id

        processing_time = 3.0  # å›ºå®šå‡¦ç†æ™‚é–“
        await asyncio.sleep(processing_time)

        # æ­£ç¢ºæ€§åˆ†æã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        accuracy_score = 85.0  # å›ºå®šã®æ­£ç¢ºæ€§ã‚¹ã‚³ã‚¢

        recommendations: list[str] = []
        if accuracy_score < 85:
            recommendations.append("Review data transformation algorithms")
        if accuracy_score < 80:
            recommendations.append("Implement additional validation steps")

        assessment = QualityAssessment(
            batch_id=batch_id,
            assessor_id=self.id,
            quality_score=accuracy_score,
            recommendations=recommendations,
            processing_time=processing_time,
        )

        await ctx.send_message(assessment)


# æœ€çµ‚å‡¦ç†ã¨å®Œäº†
class FinalProcessor(Executor):
    """ã™ã¹ã¦ã®çµæœã‚’çµåˆã™ã‚‹æœ€çµ‚å‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¸ã€‚"""

    @handler
    async def process_final_results(
        self, assessments: list[QualityAssessment], ctx: WorkflowContext[Never, str]
    ) -> None:
        """æœ€çµ‚å‡¦ç†ã®æ¦‚è¦ã‚’ç”Ÿæˆã—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Œäº†ã€‚"""
        if not assessments:
            await ctx.yield_output("No quality assessments received")
            return

        batch_id = assessments[0].batch_id

        # æœ€çµ‚å‡¦ç†é…å»¶ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        await asyncio.sleep(2)

        # å…¨ä½“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
        avg_quality_score = sum(a.quality_score for a in assessments) / len(assessments)
        total_recommendations = sum(len(a.recommendations) for a in assessments)
        total_processing_time = sum(a.processing_time for a in assessments)

        # æœ€çµ‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ±ºå®š
        if avg_quality_score >= 85:
            final_status = "EXCELLENT"
        elif avg_quality_score >= 75:
            final_status = "GOOD"
        elif avg_quality_score >= 65:
            final_status = "ACCEPTABLE"
        else:
            final_status = "NEEDS_IMPROVEMENT"

        completion_message = (
            f"Batch {batch_id} processing completed!\n"
            f"ğŸ“Š Overall Quality Score: {avg_quality_score:.1f}%\n"
            f"â±ï¸  Total Processing Time: {total_processing_time:.1f}s\n"
            f"ğŸ’¡ Total Recommendations: {total_recommendations}\n"
            f"ğŸ–ï¸  Final Status: {final_status}"
        )

        await ctx.yield_output(completion_message)


# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ“ãƒ«ãƒ€ãƒ¼ ãƒ˜ãƒ«ãƒ‘ãƒ¼
class WorkflowSetupHelper:
    """å…±æœ‰Stateç®¡ç†ã§è¤‡é›‘ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹ã€‚"""

    @staticmethod
    async def store_batch_data(batch: DataBatch, ctx: WorkflowContext) -> None:
        """å¾Œã§å–å¾—ã™ã‚‹ãŸã‚ã«å…±æœ‰Stateã«ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã€‚"""
        await ctx.set_shared_state(f"batch_{batch.batch_id}", batch)


# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
def create_complex_workflow():
    """è¤‡é›‘ãªãƒ•ã‚¡ãƒ³ã‚¤ãƒ³/ãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä½œæˆã€‚"""
    # ã™ã¹ã¦ã®Executorã‚’ä½œæˆ
    data_ingestion = DataIngestion(id="data_ingestion")

    # æ¤œè¨¼ã‚¹ãƒ†ãƒ¼ã‚¸ï¼ˆãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆï¼‰
    schema_validator = SchemaValidator(id="schema_validator")
    quality_validator = DataQualityValidator(id="quality_validator")
    security_validator = SecurityValidator(id="security_validator")
    validation_aggregator = ValidationAggregator(id="validation_aggregator")

    # å¤‰æ›ã‚¹ãƒ†ãƒ¼ã‚¸ï¼ˆãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆï¼‰
    data_normalizer = DataNormalizer(id="data_normalizer")
    data_enrichment = DataEnrichment(id="data_enrichment")
    data_aggregator_exec = DataAggregator(id="data_aggregator")

    # å“è³ªä¿è¨¼ã‚¹ãƒ†ãƒ¼ã‚¸ï¼ˆãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆï¼‰
    performance_assessor = PerformanceAssessor(id="performance_assessor")
    accuracy_assessor = AccuracyAssessor(id="accuracy_assessor")

    # æœ€çµ‚å‡¦ç†
    final_processor = FinalProcessor(id="final_processor")

    # è¤‡é›‘ãªãƒ•ã‚¡ãƒ³ã‚¤ãƒ³/ãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ§‹ç¯‰
    return (
        WorkflowBuilder(
            name="Data Processing Pipeline",
            description="Complex workflow with parallel validation, transformation, and quality assurance stages",
        )
        .set_start_executor(data_ingestion)
        # æ¤œè¨¼ã‚¹ãƒ†ãƒ¼ã‚¸ã¸ã®ãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆ
        .add_fan_out_edges(data_ingestion, [schema_validator, quality_validator, security_validator])
        # æ¤œè¨¼ã‹ã‚‰é›†ç´„å™¨ã¸ã®ãƒ•ã‚¡ãƒ³ã‚¤ãƒ³
        .add_fan_in_edges([schema_validator, quality_validator, security_validator], validation_aggregator)
        # å¤‰æ›ã‚¹ãƒ†ãƒ¼ã‚¸ã¸ã®ãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆ
        .add_fan_out_edges(validation_aggregator, [data_normalizer, data_enrichment, data_aggregator_exec])
        # å“è³ªä¿è¨¼ã‚¹ãƒ†ãƒ¼ã‚¸ã¸ã®ãƒ•ã‚¡ãƒ³ã‚¤ãƒ³ï¼ˆä¸¡æ–¹ã®è©•ä¾¡è€…ãŒã™ã¹ã¦ã®å¤‰æ›çµæœã‚’å—ä¿¡ï¼‰
        .add_fan_in_edges([data_normalizer, data_enrichment, data_aggregator_exec], performance_assessor)
        .add_fan_in_edges([data_normalizer, data_enrichment, data_aggregator_exec], accuracy_assessor)
        # æœ€çµ‚ãƒ—ãƒ­ã‚»ãƒƒã‚µã¸ã®ãƒ•ã‚¡ãƒ³ã‚¤ãƒ³
        .add_fan_in_edges([performance_assessor, accuracy_assessor], final_processor)
        .build()
    )


# DevUIæ¤œå‡ºç”¨ã«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
workflow = create_complex_workflow()


def main():
    """DevUIã§ãƒ•ã‚¡ãƒ³ã‚¢ã‚¦ãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’èµ·å‹•ã€‚"""
    from agent_framework.devui import serve

    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    logger = logging.getLogger(__name__)

    logger.info("Starting Complex Fan-In/Fan-Out Data Processing Workflow")
    logger.info("Available at: http://localhost:8090")
    logger.info("Entity ID: workflow_complex_workflow")

    # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•
    serve(entities=[workflow], port=8090, auto_open=True)


if __name__ == "__main__":
    main()
